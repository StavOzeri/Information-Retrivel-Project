{"cells": [{"cell_type": "code", "execution_count": 1, "id": "1c4479c4-e23d-4b9f-9fbc-94b8db6c7e2d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Starting Title Index Build\n"}, {"name": "stderr", "output_type": "stream", "text": "26/01/04 11:58:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Data loaded. Processing Titles\nGrouping by term\nCollecting Title Index\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Title Index Done. Saved to gs://final_project_ir_stav_hen_bucket/index_title.pkl\nTotal unique terms in titles: 1545872\n"}], "source": "import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lower, split, explode, count as spark_count\nimport pickle\nfrom google.cloud import storage\nimport math\n\nYOUR_BUCKET_NAME = 'final_project_ir_stav_hen_bucket'\n\nprint(\"Starting Title Index Build\")\n\nspark = SparkSession.builder \\\n    .appName(\"Title_Index_Build\") \\\n    .getOrCreate()\n\npath_to_data = f\"gs://{YOUR_BUCKET_NAME}/multistream*_preprocessed.parquet\"\ndf_data = spark.read.parquet(path_to_data).select(\"id\", \"title\")\n\nprint(\"Data loaded. Processing Titles\")\n\n\ndef process_titles(row):\n    doc_id = row['id']\n    title_text = row['title']\n    if title_text is None:\n        return []\n    tokens = title_text.lower().split()\n    tf_dict = {}\n    for token in tokens:\n        if token.isalnum(): \n            tf_dict[token] = tf_dict.get(token, 0) + 1\n            \n    result = []\n    for term, tf in tf_dict.items():\n        result.append((term, (doc_id, tf)))\n    return result\n\nrdd_titles = df_data.rdd.flatMap(process_titles)\n\nprint(\"Grouping by term\")\ntitle_index_rdd = rdd_titles.groupByKey().mapValues(list)\n\nprint(\"Collecting Title Index\")\ntitle_index_map = title_index_rdd.collectAsMap()\n\nlocal_filename = 'index_title.pkl'\nwith open(local_filename, 'wb') as f:\n    pickle.dump(title_index_map, f)\n\nclient = storage.Client()\nblob = client.bucket(YOUR_BUCKET_NAME).blob(local_filename)\nblob.upload_from_filename(local_filename)\n\nprint(f\"Title Index Done. Saved to gs://{YOUR_BUCKET_NAME}/{local_filename}\")\nprint(f\"Total unique terms in titles: {len(title_index_map)}\")"}, {"cell_type": "code", "execution_count": 1, "id": "1c470467-851f-4fc9-a515-05fdc2018b2b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "26/01/04 14:15:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Data loaded. Creating ID to Title Dictionary\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Success! Saved id_to_title.pkl to gs://final_project_ir_stav_hen_bucket/postings_gcp/id_to_title.pkl\nTotal documents mapped: 6348910\n"}], "source": "import pyspark\nfrom pyspark.sql import SparkSession\nimport pickle\nfrom google.cloud import storage\nimport os\n\nYOUR_BUCKET_NAME = 'final_project_ir_stav_hen_bucket'\n\nspark = SparkSession.builder.appName(\"Create_ID_to_Title_Map\").getOrCreate()\n\npath_to_data = f\"gs://{YOUR_BUCKET_NAME}/multistream*_preprocessed.parquet\"\ndf_data = spark.read.parquet(path_to_data).select(\"id\", \"title\")\n\nprint(\"Data loaded. Creating ID to Title Dictionary\")\n\nid_to_title_map = df_data.rdd.map(lambda x: (x.id, x.title)).collectAsMap()\n\nlocal_filename = 'id_to_title.pkl'\nwith open(local_filename, 'wb') as f:\n    pickle.dump(id_to_title_map, f)\n\nclient = storage.Client()\nblob = client.bucket(YOUR_BUCKET_NAME).blob(f'postings_gcp/{local_filename}')\nblob.upload_from_filename(local_filename)\n\nprint(f\"Success! Saved id_to_title.pkl to gs://{YOUR_BUCKET_NAME}/postings_gcp/{local_filename}\")\nprint(f\"Total documents mapped: {len(id_to_title_map)}\")"}, {"cell_type": "code", "execution_count": null, "id": "4a4613c9-c651-4784-813d-a705b19882fb", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}