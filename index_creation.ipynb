{"cells": [{"cell_type": "code", "execution_count": null, "id": "687633dc-d2fe-47de-94b8-fbbb80cb84b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"}, {"name": "stdout", "output_type": "stream", "text": "Reading Wiki data from: gs://final_project_ir_stav_hen_bucket/*.parquet\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Step 1/4: Building Title Index (Mapping Doc IDs to Titles)\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Title Index saved successfully\nStep 2/4: Processing Body Text (Calculating TF, DF, and Norms)\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Step 3/4: Calculating Document Norms\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Document Norms saved successfully.\nStep 4/4: Writing Posting Lists to GCP Storage\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Finalizing: Consolidating and Saving the InvertedIndex object\nAll indexes created and saved successfully\n"}], "source": "import pyspark\nimport sys\nfrom collections import Counter, OrderedDict, defaultdict\nimport itertools\nfrom itertools import islice, count, groupby\nimport pandas as pd\nimport os\nimport re\nfrom operator import itemgetter\nfrom time import time\nfrom pathlib import Path\nimport pickle\nfrom google.cloud import storage\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark import SparkContext, SparkConf, SparkFiles\nfrom pyspark.sql import SQLContext\nimport math\nimport builtins \nfrom contextlib import closing\n\n# configuration and setup\nYOUR_PROJECT_BUCKET_NAME_STRING = 'final_project_ir_stav_hen_bucket' \ngoogle_storage_client = storage.Client()\npath_to_local_helper_file = \"/hadoop/cms/jupyter/user_home/inverted_index_gcp.py\"\nif os.path.exists(path_to_local_helper_file):\n    sys.path.insert(0, \"/hadoop/cms/jupyter/user_home/\")\nelse:\n    list_of_blobs_in_bucket = list(google_storage_client.list_blobs(YOUR_PROJECT_BUCKET_NAME_STRING, prefix='inverted_index_gcp.py'))\n    if len(list_of_blobs_in_bucket) > 0:\n        blob_object_reference = list_of_blobs_in_bucket[0]\n        blob_object_reference.download_to_filename('inverted_index_gcp.py')\n        sys.path.insert(0, os.getcwd())\nsc.addFile(\"inverted_index_gcp.py\")\nfrom inverted_index_gcp import InvertedIndex, MultiFileWriter\n\n# tokenization-\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nenglish_language_stopwords = frozenset(stopwords.words('english'))\ncorpus_specific_stopwords_list = [\"category\", \"references\", \"also\", \"external\", \"links\",\n                                  \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n                                  \"part\", \"thumb\", \"including\", \"second\", \"following\",\n                                  \"many\", \"however\", \"would\", \"became\"]\ncombined_stopwords_set = english_language_stopwords.union(corpus_specific_stopwords_list)\nREGEX_PATTERN_FOR_TOKENS = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n\ndef extract_tokens_from_text_string(input_text_string):\n    \"\"\"Parses the text and returns a list of valid tokens after filtering stopwords.\"\"\"\n    list_of_raw_tokens = [token_match.group() for token_match in REGEX_PATTERN_FOR_TOKENS.finditer(input_text_string.lower())]\n    return [token_str for token_str in list_of_raw_tokens if token_str not in combined_stopwords_set]\n\ndef calculate_term_frequencies_for_document(text_content, document_unique_id):\n    \"\"\"Tokenizes text and returns pairs of (token, (doc_id, tf)).\"\"\"\n    list_of_document_tokens = extract_tokens_from_text_string(text_content)\n    term_frequency_counter = Counter(list_of_document_tokens)\n    return [(token_str, (document_unique_id, term_count)) for token_str, term_count in term_frequency_counter.items()]\n\ndef sort_posting_list_tuples(unsorted_posting_list_iterable):\n    \"\"\"Sorts a posting list by document ID.\"\"\"\n    return sorted(list(unsorted_posting_list_iterable), key=lambda x: x[0])\n\ndef calculate_document_frequency(posting_entry_tuple):\n    \"\"\"Calculates in how many documents each term appears (DF).\"\"\"\n    term_string, posting_list_data = posting_entry_tuple\n    return (term_string, len(posting_list_data))\n\n# storage logic\nNUMBER_OF_BUCKETS_FOR_SHARDING = 124\nimport hashlib\n\ndef map_token_string_to_bucket_id(token_string):\n    \"\"\"Maps a token to a bucket ID for sharding.\"\"\"\n    return int(hashlib.blake2b(bytes(token_string, encoding='utf8'), digest_size=5).hexdigest(), 16) % NUMBER_OF_BUCKETS_FOR_SHARDING\n\nSIZE_OF_TUPLE_IN_BYTES = 6       \nBIT_MASK_FOR_TF = 2**16 - 1 \n\ndef write_data_to_gcp_robustly_handling_blob_errors(bucket_data_tuple):\n    \"\"\"\n    Writes data to local worker disk first, then uploads to GCS.\n    This bypasses the 'Blob.open' error on older GCP libraries.\n    \"\"\"\n    bucket_id_integer, items_iterator = bucket_data_tuple\n    local_temporary_directory_path = Path(f\"/tmp/postings_gcp_{bucket_id_integer}\")\n    if not local_temporary_directory_path.exists():\n        local_temporary_directory_path.mkdir(parents=True, exist_ok=True)\n    dictionary_of_posting_locations = defaultdict(list)\n    with closing(MultiFileWriter(local_temporary_directory_path, bucket_id_integer, bucket_name=None)) as local_file_writer:\n        for term_string, posting_list_data in items_iterator: \n            binary_data = b''.join([(doc_id << 16 | (tf_count & BIT_MASK_FOR_TF)).to_bytes(SIZE_OF_TUPLE_IN_BYTES, 'big')\n                          for doc_id, tf_count in posting_list_data])\n            locations_written = local_file_writer.write(binary_data)\n            dictionary_of_posting_locations[term_string].extend(locations_written)\n    worker_node_storage_client = storage.Client()\n    target_bucket_object = worker_node_storage_client.bucket(YOUR_PROJECT_BUCKET_NAME_STRING)\n    for local_binary_file in local_temporary_directory_path.glob(\"*.bin\"):\n        remote_blob_path = f\"postings_gcp/{local_binary_file.name}\"\n        blob_object = target_bucket_object.blob(remote_blob_path)\n        blob_object.upload_from_filename(str(local_binary_file))\n        local_binary_file.unlink() # Delete local file after upload to save space\n    pickle_filename_string = f'{bucket_id_integer}_posting_locs.pickle'\n    local_pickle_file_path = local_temporary_directory_path / pickle_filename_string\n    with open(local_pickle_file_path, 'wb') as open_pickle_file:\n        pickle.dump(dictionary_of_posting_locations, open_pickle_file)\n    remote_pickle_blob = target_bucket_object.blob(f\"postings_gcp/{pickle_filename_string}\")\n    remote_pickle_blob.upload_from_filename(str(local_pickle_file_path))\n    local_pickle_file_path.unlink()\n    try:\n        local_temporary_directory_path.rmdir()\n    except:\n        pass\n        \n    return bucket_id_integer\n\ndef partition_and_write_postings_to_cloud_storage(rdd_of_filtered_postings):\n    \"\"\"Groups postings by bucket ID and writes them to GCP storage using the robust method.\"\"\"\n    rdd_mapped_to_buckets = rdd_of_filtered_postings.map(lambda x: (map_token_string_to_bucket_id(x[0]), (x[0], x[1])))\n    rdd_grouped_by_bucket_id = rdd_mapped_to_buckets.groupByKey()\n    return rdd_grouped_by_bucket_id.map(write_data_to_gcp_robustly_handling_blob_errors).collect()\n\n# main execution\nfull_gcs_path_to_parquet = f\"gs://{YOUR_PROJECT_BUCKET_NAME_STRING}/*.parquet\"\nblobs_check_list = list(google_storage_client.list_blobs(YOUR_PROJECT_BUCKET_NAME_STRING, prefix='wikidata20210801_preprocessed'))\nif len(blobs_check_list) > 0:\n     full_gcs_path_to_parquet = f\"gs://{YOUR_PROJECT_BUCKET_NAME_STRING}/wikidata20210801_preprocessed/*.parquet\"\n\nprint(f\"Reading Wiki data from: {full_gcs_path_to_parquet}\")\nspark_dataframe_wiki_data = spark.read.parquet(full_gcs_path_to_parquet)\n\n# Title Index\nprint(\"Step 1/4: Building Title Index (Mapping Doc IDs to Titles)\")\nrdd_ids_and_titles = spark_dataframe_wiki_data.select(\"id\", \"title\").rdd\ndictionary_id_to_title_map = rdd_ids_and_titles.collectAsMap()\n\n# Save locally\nwith open('index_title.pkl', 'wb') as title_index_file:\n    pickle.dump(dictionary_id_to_title_map, title_index_file)\ngoogle_storage_client.bucket(YOUR_PROJECT_BUCKET_NAME_STRING).blob('index_title.pkl').upload_from_filename('index_title.pkl')\nprint(\"Title Index saved successfully\")\n\n# Body Index\nprint(\"Step 2/4: Processing Body Text (Calculating TF, DF, and Norms)\")\nrdd_text_and_doc_id = spark_dataframe_wiki_data.select(\"text\", \"id\").rdd\n\n# Calculate TF\nrdd_flat_word_counts = rdd_text_and_doc_id.flatMap(lambda row: calculate_term_frequencies_for_document(row[0], row[1]))\n\n# Group by Word and Sort\nrdd_grouped_postings = rdd_flat_word_counts.groupByKey().mapValues(sort_posting_list_tuples)\n\n# Filter Rare Words\nrdd_filtered_postings = rdd_grouped_postings.filter(lambda x: len(x[1]) > 50)\n\n# Calculate Document Frequency\nrdd_document_frequency_pairs = rdd_filtered_postings.map(calculate_document_frequency)\ndictionary_term_document_frequency = rdd_document_frequency_pairs.collectAsMap()\n\ndef calculate_norm_for_single_doc(doc_pair_data):\n    text_content, doc_id_val = doc_pair_data\n    doc_tokens = extract_tokens_from_text_string(text_content)\n    token_counter = Counter(doc_tokens)\n    norm_value = math.sqrt(builtins.sum([freq_val**2 for freq_val in token_counter.values()]))\n    return (doc_id_val, norm_value)\n\nprint(\"Step 3/4: Calculating Document Norms\")\ndictionary_document_norms = rdd_text_and_doc_id.map(calculate_norm_for_single_doc).collectAsMap()\n\n# Save Norms\nwith open('index_norms.pkl', 'wb') as norms_file_obj:\n    pickle.dump(dictionary_document_norms, norms_file_obj)\ngoogle_storage_client.bucket(YOUR_PROJECT_BUCKET_NAME_STRING).blob('index_norms.pkl').upload_from_filename('index_norms.pkl')\nprint(\"Document Norms saved successfully.\")\n\n# Write Index to Storage\nprint(\"Step 4/4: Writing Posting Lists to GCP Storage\")\n_ = partition_and_write_postings_to_cloud_storage(rdd_filtered_postings)\n\n# Finalize and Save Index Object\nprint(\"Finalizing: Consolidating and Saving the InvertedIndex object\")\n\ndefaultdict_super_posting_locs = defaultdict(list)\niterator_all_blobs = google_storage_client.list_blobs(YOUR_PROJECT_BUCKET_NAME_STRING, prefix='postings_gcp')\n\n# Temporary file for robust downloading\ntemporary_download_filename = \"temp_robust_download.pkl\"\n\nfor blob_item in iterator_all_blobs:\n    if not blob_item.name.endswith(\"pickle\"):\n        continue\n    try:\n        blob_item.download_to_filename(temporary_download_filename)\n        with open(temporary_download_filename, \"rb\") as open_pickle_file:\n            partial_posting_locs_data = pickle.load(open_pickle_file)\n            for term_key, locations_list in partial_posting_locs_data.items():\n                defaultdict_super_posting_locs[term_key].extend(locations_list)\n    except Exception as e:\n        print(f\"Skipping blob {blob_item.name} due to error: {e}\")\n    finally:\n        if os.path.exists(temporary_download_filename):\n            os.remove(temporary_download_filename)\n\n# Create the final InvertedIndex object\nfinal_inverted_index_object = InvertedIndex()\nfinal_inverted_index_object.posting_locs = defaultdict_super_posting_locs\nfinal_inverted_index_object.df = dictionary_term_document_frequency\nfinal_inverted_index_object.write_index('.', 'index_body')\n\n# Upload the final object\nlocal_index_filename = \"index_body.pkl\"\nremote_destination_path = f'postings_gcp/{local_index_filename}'\ngoogle_storage_client.bucket(YOUR_PROJECT_BUCKET_NAME_STRING).blob(remote_destination_path).upload_from_filename(local_index_filename)\n\nprint(\"All indexes created and saved successfully\")"}, {"cell_type": "code", "execution_count": null, "id": "01fd25c9-50a9-4b7d-a7e7-43e456f1a08a", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}